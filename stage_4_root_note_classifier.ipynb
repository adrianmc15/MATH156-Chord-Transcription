{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy.io.wavfile import read as read_wav\n",
    "from scipy import fft,signal\n",
    "from sklearn.preprocessing import minmax_scale\n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "\n",
    "TONE_A = 440 \n",
    "NOTES = ['A','A#','B','C','C#','D','D#','E','F','F#','G','G#'] \n",
    "\n",
    "# feature extraction on intervals of the first two chords\n",
    "# find the top 10 most frequent intervals\n",
    "# use the top 10 most frequent intervals as features\n",
    "\n",
    "def freq_to_rnote(freq):\n",
    "    r = 12.0*np.log2(freq/TONE_A)\n",
    "    return r\n",
    "\n",
    "def rnote_to_freq(r):\n",
    "    f = TONE_A*2**(r/12)\n",
    "    return f\n",
    "\n",
    "def get_note_volume(rnote,fft_image,fft_freq,rnote_epsilon=0.2):\n",
    "    \"\"\" rnote - name or number of note,fft_image - fourier image of signal,\n",
    "    fft_freq - frequencies in fft_image,rnote_epsilon - halfwide of window to inspect\n",
    "    return maximum volume(magnitude) of signal in freq window for rnote \"\"\"\n",
    "    if isinstance(rnote,str):\n",
    "        rnote = NOTES.index(rnote)\n",
    "    try:\n",
    "        f0 = rnote_to_freq(rnote-rnote_epsilon)\n",
    "        f1 = rnote_to_freq(rnote+rnote_epsilon)\n",
    "        f_idx = np.where((fft_freq>=f0)&(fft_freq<=f1)) \n",
    "        maxVol = np.max((fft_image[f_idx]))\n",
    "    except Exception:\n",
    "        return 0.\n",
    "    \n",
    "    return maxVol\n",
    "\n",
    "def get_notes_volume(rnote,fft_image,fft_freq,rnote_epsilon=0.5,oct_range_from=-4.,oct_range_to=8.):\n",
    "    if isinstance(rnote,str):\n",
    "        rnote = NOTES.index(rnote)\n",
    "    rnotes = np.arange(rnote+12.*oct_range_from,rnote+12.*oct_range_to,12.0)\n",
    "    vol = []\n",
    "    for rn in rnotes:\n",
    "        vol.append(get_note_volume(rn,fft_image,fft_freq))\n",
    "        \n",
    "    return np.max(vol)\n",
    "\n",
    "def chord_quality(fileName):\n",
    "    rate, data_raw = read_wav(fileName)\n",
    "    data = (data_raw[:,0]+data_raw[:,1]).astype(np.float32) # stereo of any type -> mono of float32\n",
    "    data = minmax_scale(data,(-1.,1.))\n",
    "    fft_image = np.abs(fft.rfft(data,norm='forward'))\n",
    "    fft_freq = fft.rfftfreq(len(data),1./rate)\n",
    "\n",
    "    major_vol = []\n",
    "    minor_vol = []\n",
    "    diminished_vol = []\n",
    "    augmented_vol = []\n",
    "\n",
    "    for rnote in range(12):\n",
    "        vol = get_notes_volume(rnote,fft_image,fft_freq)\n",
    "        v3 = get_notes_volume((rnote+3)%12,fft_image,fft_freq)\n",
    "        v4 = get_notes_volume((rnote+4)%12,fft_image,fft_freq)\n",
    "        v6 = get_notes_volume((rnote+6)%12,fft_image,fft_freq)\n",
    "        v7 = get_notes_volume((rnote+7)%12,fft_image,fft_freq)\n",
    "        v8 = get_notes_volume((rnote+8)%12,fft_image,fft_freq)\n",
    "\n",
    "        major_vol.append(vol + v4 + v7)\n",
    "        minor_vol.append(vol + v3 + v7)\n",
    "        diminished_vol.append(vol + v3 + v6)\n",
    "        augmented_vol.append(vol + v4 + v8)\n",
    "    \n",
    "    major_max = [max(major_vol), NOTES[major_vol.index(max(major_vol))]]\n",
    "    minor_max = [max(minor_vol), NOTES[minor_vol.index(max(minor_vol))]] \n",
    "    diminished_max = [max(diminished_vol), NOTES[diminished_vol.index(max(diminished_vol))]] \n",
    "    augmented_max = [max(augmented_vol), NOTES[augmented_vol.index(max(augmented_vol))]] \n",
    "\n",
    "    if max(major_max[0], minor_max[0], diminished_max[0], augmented_max[0]) == major_max[0]:\n",
    "        return 'maj', major_max[1]\n",
    "    elif max(major_max[0], minor_max[0], diminished_max[0], augmented_max[0]) == minor_max[0]:\n",
    "        return 'min', minor_max[1]\n",
    "    elif max(major_max[0], minor_max[0], diminished_max[0], augmented_max[0]) == diminished_max[0]:\n",
    "        return 'dim', diminished_max[1]\n",
    "    else:\n",
    "        return 'aug', augmented_max[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "# Load your chord classification model\n",
    "chord_classification_model = load_chord_classification_model()  # Implement this function\n",
    "\n",
    "# Define the CNN model for root note prediction with multiple inputs\n",
    "input_spectrogram = layers.Input(shape=(161, 1501, 1), name='input_spectrogram')\n",
    "input_chord_classification = layers.Input(shape=(4,), name='input_chord_classification')  # Assuming 4 chord classes\n",
    "\n",
    "# Spectrogram processing\n",
    "x = layers.Conv2D(32, (3, 3), activation='relu')(input_spectrogram)\n",
    "x = layers.MaxPooling2D((2, 2))(x)\n",
    "x = layers.Conv2D(64, (3, 3), activation='relu')(x)\n",
    "x = layers.MaxPooling2D((2, 2))(x)\n",
    "x = layers.Flatten()(x)\n",
    "\n",
    "# Chord classification processing\n",
    "y = layers.Dense(16, activation='relu')(input_chord_classification)\n",
    "\n",
    "# Concatenate the outputs of both paths\n",
    "concatenated = layers.concatenate([x, y])\n",
    "\n",
    "# Additional dense layers for joint processing\n",
    "z = layers.Dense(128, activation='relu')(concatenated)\n",
    "z = layers.Dropout(0.5)(z)\n",
    "z = layers.Dense(12, activation='softmax')(z)  # Assuming 12 root notes\n",
    "\n",
    "# Create the model\n",
    "model = models.Model(inputs=[input_spectrogram, input_chord_classification], outputs=z)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model using your dataset\n",
    "model.fit({'input_spectrogram': root_note_data_spectrogram, 'input_chord_classification': root_note_data_chord_classification},\n",
    "          root_note_labels, epochs=10, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
