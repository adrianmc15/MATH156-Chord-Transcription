{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing:\n",
    "def preprocess(fileName):\n",
    "    rate, data_raw = read_wav(fileName)\n",
    "    data = (data_raw[:,0]+data_raw[:,1]).astype(np.float32) # stereo of any type -> mono of float32\n",
    "    data = minmax_scale(data,(-1.,1.))\n",
    "    fft_image = np.abs(fft.rfft(data,norm='forward'))\n",
    "    fft_freq = fft.rfftfreq(len(data),1./rate)\n",
    "    note_volumes = []\n",
    "    for note in NOTES:        \n",
    "    \n",
    "        note_volumes.append(get_note_volume(note,fft_image,fft_freq))\n",
    "    # print(note_volumes)\n",
    "    \n",
    "    return note_volumes\n",
    "\n",
    "# create a new dataframe with the note volumes\n",
    "train_note_volumes = pd.DataFrame(columns=NOTES)\n",
    "test_note_volumes = pd.DataFrame(columns=NOTES)    \n",
    "\n",
    "for i in tqdm(range(len(train_set))):\n",
    "    fileName = train_set['File Path'].iloc[i]\n",
    "    note_volumes = preprocess(fileName)\n",
    "    # print the shape of note_volumes\n",
    "    # print(note_volumes.shape)\n",
    "    for j in range(len(note_volumes)):\n",
    "        train_note_volumes.at[i, NOTES[j]] = note_volumes[j]\n",
    "\n",
    "for i in tqdm(range(len(test_set))):\n",
    "    fileName = test_set['File Path'].iloc[i]\n",
    "    note_volumes = preprocess(fileName)\n",
    "    for j in range(len(note_volumes)):\n",
    "        test_note_volumes.at[i, NOTES[j]] = note_volumes[j]\n",
    "        # test_set.iloc[i][NOTES[j]] = note_volumes[j]\n",
    "        # test_set.at[i, NOTES[j]] = note_volumes[j]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# summarise the quality column in train_set\n",
    "train_set['Quality'].describe()\n",
    "\n",
    "# show each quality in train_set and how many there are\n",
    "train_set.groupby(['Quality']).count()\n",
    "\n",
    "\n",
    "# how many nan values are there in the train_set?\n",
    "\n",
    "# add the qualities to the note_volumes dataframe\n",
    "train_note_volumes['Quality'] = train_set['Quality']\n",
    "test_note_volumes['Quality'] = test_set['Quality']\n",
    "\n",
    "len(train_note_volumes)\n",
    "\n",
    "\n",
    "# print(train_set)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "# Assuming df is your DataFrame and it's already been normalized row-wise\n",
    "\n",
    "# Create a scaler object\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Fit and transform the data but not for the 'File Path', Inversion, Root Note, Quality, and Chord Name columns\n",
    "\n",
    "columns_to_scale = train_set.drop(columns=['File Path', 'Quality', 'Root Note', 'Octave', 'Inversion']).columns\n",
    "train_set[columns_to_scale] = scaler.fit_transform(train_set[columns_to_scale])\n",
    "\n",
    "print(train_set.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_note_volumes['Quality'].unique())\n",
    "print(len(train_set))\n",
    "# remove the chords that have nan values in train_set\n",
    "train_set = train_set.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run a CNN on the data, using the note volumes as the input and the chord quality as the output\n",
    "# Path: stage_3_cnn_classifier_part_2.ipynb\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv1D, MaxPooling1D, Flatten, Dense\n",
    "\n",
    "num_classes = 4\n",
    "height = 12\n",
    "width = 1\n",
    "channels = 1\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# Add Convolutional layers\n",
    "model.add(Conv1D(32, 3, activation='relu', input_shape=(height, channels)))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "\n",
    "model.add(Conv1D(32, 3, activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "\n",
    "# Flatten the output to feed into dense layers\n",
    "model.add(Flatten())\n",
    "\n",
    "# Add Dense layers\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv1D, MaxPooling1D, Flatten, Dense\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Assuming you have already loaded train_note_volumes and labels from your data\n",
    "# Make sure to convert labels to categorical\n",
    "\n",
    "# Convert labels to categorical one-hot encoding\n",
    "labels_categorical = to_categorical(train_note_volumes['Quality'], num_classes=4)\n",
    "\n",
    "# make input_data a combination of train_note_volumes and test_note_volumes\n",
    "\n",
    "input_data = np.concatenate((train_note_volumes.drop(columns=['Quality']), test_note_volumes.drop(columns=['Quality'])), axis=0)\n",
    "\n",
    "# Reshape input data to have a channel dimension\n",
    "input_data = input_data.reshape(input_data.shape[0], input_data.shape[1], 1)\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(input_data, labels_categorical, test_size=0.2, random_state=42)\n",
    "\n",
    "# Build a simple CNN model\n",
    "model = Sequential()\n",
    "model.add(Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(input_data.shape[1], 1)))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(4, activation='softmax'))  # For multi-class classification with 4 classes\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Fit the model\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_val, y_val))\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(input_data, labels_categorical)\n",
    "print(f'Test Loss: {loss}, Test Accuracy: {accuracy}')\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
